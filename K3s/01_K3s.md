Hier ist eine umfassende Gliederung f√ºr ein Tutorial √ºber Kubernetes mit **K3s**, das von Grundlagen bis zu fortgeschrittenen Themen reicht:

---

# **Tutorial: Kubernetes mit K3s ‚Äì Von den Grundlagen zur Produktionsreife**

## **1. Einf√ºhrung in Kubernetes und K3s**
   - **1.1 Was ist Kubernetes?**
     - Container-Orchestrierung erkl√§rt
     - Kubernetes-Architektur (Master/Worker Nodes, Pods, Services, etc.)
   - **1.2 Warum K3s?**
     - Lightweight-Kubernetes f√ºr Edge, IoT und Entwicklungsmaschinen
     - Unterschiede zwischen K3s und K8s (Gr√∂√üe, Komplexit√§t, Ressourcenverbrauch)
   - **1.3 Anwendungsf√§lle f√ºr K3s**
     - Lokale Entwicklung
     - Edge Computing
     - CI/CD-Pipelines

## **2. Installation und Einrichtung von K3s**
   - **2.1 Systemvoraussetzungen**
     - Hardware- und OS-Anforderungen
   - **2.2 Installation auf verschiedenen Plattformen**
     - Linux (Ubuntu/Debian, CentOS/RHEL)
     - macOS (mit Multipass oder Docker)
     - Windows (WSL2)
   - **2.3 K3s mit einem Befehl installieren**
     - Single-Node-Cluster (`curl -sfL https://get.k3s.io | sh -`)
     - Multi-Node-Cluster (Agent-Modus)
   - **2.4 K3s ohne Root-Rechte (MicroK8s-Vergleich)**
   - **2.5 K3s mit Docker oder containerd**

## **3. Grundlegende K3s-Konzepte und -Befehle**
   - **3.1 Kubectl-Grundlagen**
     - Konfiguration (`~/.kube/config`)
     - Wichtige Befehle (`get`, `describe`, `logs`, `exec`)
   - **3.2 K3s-spezifische Komponenten**
     - Embedded SQLite statt etcd
     - `k3s-server` und `k3s-agent`
   - **3.3 Deployment von Pods und Services**
     - YAML-Manifeste erstellen
     - `kubectl apply -f deployment.yaml`
   - **3.4 Helm unter K3s nutzen**
     - Installation und grundlegende Charts

## **4. Netzwerk und Storage in K3s**
   - **4.1 Netzwerkkonfiguration**
     - Service Mesh (Traefik als Standard-Ingress)
     - LoadBalancer mit `klipper-lb` (MetalLB-Alternative)
   - **4.2 Persistent Storage**
     - Local Path Provisioner
     - NFS/CSI-Treiber einbinden
   - **4.3 DNS und Service Discovery**

## **5. Sicherheit in K3s**
   - **5.1 RBAC (Role-Based Access Control)**
   - **5.2 TLS und Zertifikatsverwaltung**
   - **5.3 Network Policies**
   - **5.4 K3s-H√§rtung f√ºr Produktionsumgebungen**

## **6. Skalierung und Hochverf√ºgbarkeit (HA)**
   - **6.1 Multi-Server-Cluster aufbauen**
     - Datenbank-Backend (MySQL/PostgreSQL statt SQLite)
   - **6.2 Auto-Scaling mit K3s**
     - Horizontal Pod Autoscaler (HPA)
   - **6.3 Backup und Disaster Recovery**

## **7. Monitoring und Logging**
   - **7.1 Prometheus & Grafana einrichten**
   - **7.2 Loki & Fluentd f√ºr Logs**
   - **7.3 K3s-Dashboard (Kubernetes Dashboard oder Rancher UI)**

## **8. CI/CD mit K3s**
   - **8.1 GitOps mit ArgoCD/Flux**
   - **8.2 Automatische Deployment-Pipelines (GitHub Actions, GitLab CI)**
   - **8.3 Canary- und Blue/Green-Deployments**

## **9. Troubleshooting und Optimierung**
   - **9.1 H√§ufige Fehler und L√∂sungen**
   - **9.2 Ressourcenlimits und Optimierung**
   - **9.3 Debugging-Tools (k9s, Lens, Octant)**

## **10. Praxisbeispiele**
   - **10.1 WordPress auf K3s deployen**
   - **10.2 Stateful Application (MySQL mit Persistent Volume)**
   - **10.3 Machine Learning mit K3s (Kubeflow Lite)**

## **11. Ausblick**
   - **11.1 K3d (K3s in Docker) f√ºr Entwicklung**
   - **11.2 Rancher-Integration**
   - **11.3 Kubernetes Edge Computing mit K3s**

---

### **Anhang**
   - **A. N√ºtzliche Befehle & Cheatsheet**
   - **B. Weiterf√ºhrende Links & Dokumentation**
   - **C. Vergleich: K3s vs. MicroK8s vs. Minikube vs. Kind**

---

Diese Gliederung bietet einen **strukturierten Lernpfad** von den Grundlagen bis zu fortgeschrittenen Themen. Je nach Zielgruppe (Anf√§nger, DevOps, Entwickler) k√∂nnen einzelne Kapitel vertieft oder gek√ºrzt werden.  

**Falls du spezielle Schwerpunkte m√∂chtest (z. B. mehr Security oder bestimmte Use Cases), lass es mich wissen!** üöÄ




# **1. Einf√ºhrung in Kubernetes und K3s**  

## **1.1 Was ist Kubernetes?**  
Kubernetes (oft als **K8s** abgek√ºrzt) ist ein Open-Source-Container-Orchestrierungssystem, das urspr√ºnglich von Google entwickelt und sp√§ter der Cloud Native Computing Foundation (CNCF) √ºbergeben wurde. Es dient zur Automatisierung der Bereitstellung, Skalierung und Verwaltung von containerisierten Anwendungen.  

### **Container-Orchestrierung erkl√§rt**  
Containerisierung (z. B. mit Docker) erm√∂glicht die Isolation von Anwendungen in leichtgewichtigen, portablen Umgebungen. Kubernetes orchestriert diese Container, indem es folgende Aufgaben √ºbernimmt:  
- **Automatische Bereitstellung** von Containern auf einer Cluster-Infrastruktur.  
- **Lastverteilung (Load Balancing)** zwischen Containern.  
- **Selbstheilung** bei Ausf√§llen (Neustart von Containern, Ersatz defekter Nodes).  
- **Skalierung** je nach Auslastung (horizontal und vertikal).  
- **Ressourcenmanagement** (CPU, RAM, Storage).  

### **Kubernetes-Architektur**  
Ein Kubernetes-Cluster besteht aus zwei Hauptkomponenten:  

#### **Master Node (Control Plane)**  
- **API-Server (kube-apiserver):** Dient als Schnittstelle f√ºr Verwaltungsbefehle.  
- **Scheduler (kube-scheduler):** Weist Pods den Worker Nodes zu.  
- **Controller Manager (kube-controller-manager):** √úberwacht den Clusterzustand.  
- **etcd:** Hochverf√ºgbarer Key-Value-Speicher f√ºr Cluster-Daten.  

#### **Worker Nodes**  
- **kubelet:** Agent, der mit der Control Plane kommuniziert.  
- **kube-proxy:** Verwaltet Netzwerkregeln f√ºr Pod-Kommunikation.  
- **Container Runtime (Docker, containerd, CRI-O):** F√ºhrt die Container aus.  

#### **Weitere Schl√ºsselkonzepte**  
- **Pods:** Kleinste Einheit in Kubernetes (kann einen oder mehrere Container enthalten).  
- **Services:** Stabile Netzwerk-Endpunkte f√ºr Pods.  
- **Deployments:** Verwaltet Pod-Replikate und Updates.  
- **Ingress:** Externer Zugriff auf Services.  

---

## **1.2 Warum K3s?**  
K3s ist eine **leichtgewichtige, optimierte Kubernetes-Distribution**, entwickelt von Rancher Labs (jetzt SUSE). Es ist speziell f√ºr ressourcenbeschr√§nkte Umgebungen wie Edge Computing, IoT oder lokale Entwicklungsumgebungen konzipiert.  

### **Vorteile von K3s gegen√ºber Standard-Kubernetes**  
- **Geringerer Ressourcenverbrauch:**  
  - Arbeitet bereits mit **512 MB RAM** und **1 CPU**.  
  - Ideal f√ºr Raspberry Pi, kleine VMs oder Laptops.  
- **Vereinfachte Installation:**  
  - Einzeilige Installation (`curl -sfL https://get.k3s.io | sh -`).  
  - Keine komplexe Konfiguration n√∂tig.  
- **Embedded Komponenten:**  
  - **SQLite statt etcd** als Standard-Datenbank (optional: MySQL/PostgreSQL).  
  - **Traefik als integrierter Ingress-Controller**.  
  - **klipper-lb** als Lightweight-LoadBalancer.  
- **Keine Abh√§ngigkeit von Cloud-Anbietern:**  
  - Funktioniert auch auf Bare-Metal und Edge-Ger√§ten.  

### **Nachteile von K3s**  
- **Nicht f√ºr gro√üe Produktionscluster** mit Hunderten von Nodes geeignet.  
- **Eingeschr√§nkte High-Availability (HA)** ohne externe Datenbank.  

---

## **1.3 Anwendungsf√§lle f√ºr K3s**  

### **Lokale Entwicklung**  
- Schnelles Erstellen von Testclustern auf dem Laptop.  
- Ideal f√ºr Entwickler, die Kubernetes ohne Overhead nutzen m√∂chten.  

### **Edge Computing & IoT**  
- **Raspberry Pi-Cluster** f√ºr dezentrale Anwendungen.  
- **Industrielle IoT-Ger√§te** mit begrenzter Hardware.  

### **CI/CD-Pipelines**  
- **Kurzlebige Testcluster** f√ºr automatisiertes Deployment.  
- Integration mit GitHub Actions, GitLab CI oder Jenkins.  

### **Homelab und Experimente**  
- Kosteng√ºnstige M√∂glichkeit, Kubernetes zu lernen.  
- Ideal f√ºr Selbsthosting (Nextcloud, Heimautomatisierung).  

---

Dieser Abschnitt bietet eine fundierte Grundlage, um die Bedeutung von Kubernetes und die spezifischen Vorteile von K3s zu verstehen.



# **2. Installation und Einrichtung von K3s**  

## **2.1 Systemvoraussetzungen**  
Bevor K3s installiert wird, m√ºssen die folgenden Mindestanforderungen erf√ºllt sein:  

### **Hardware-Anforderungen**  
- **CPU:** Mindestens 1 Kern (empfohlen: 2 Kerne f√ºr bessere Leistung).  
- **RAM:** Minimum **512 MB** (empfohlen: 1 GB f√ºr Produktivumgebungen).  
- **Speicher:** Mindestens **100 MB** f√ºr K3s-Bin√§rdateien, zus√§tzlicher Platz f√ºr Container und Daten.  

### **Betriebssystem-Unterst√ºtzung**  
- **Linux:**  
  - Ubuntu (18.04, 20.04, 22.04)  
  - Debian (10, 11)  
  - CentOS / RHEL (7, 8)  
  - Fedora (34+)  
  - Arch Linux  
  - openSUSE  
- **macOS:** Nutzbar √ºber Multipass, Docker oder Lima.  
- **Windows:** Unterst√ºtzt via **WSL2** (Windows Subsystem for Linux).  

### **Netzwerkanforderungen**  
- **Internetverbindung** zum Herunterladen von K3s und Containern.  
- **Statische IP oder DHCP** f√ºr stabile Cluster-Kommunikation.  
- **Firewall:** Ports **6443 (Kubernetes API)** und **8472 (Flannel VXLAN)** m√ºssen freigegeben sein.  

---

## **2.2 Installation auf verschiedenen Plattformen**  

### **Linux (Ubuntu/Debian als Beispiel)**  
#### **Automatische Installation (Einzeiler)**  
```bash
curl -sfL https://get.k3s.io | sh -
```  
- Installiert K3s als **Server (Master Node)**.  
- Aktiviert den **systemd-Service (`k3s.service`)** f√ºr automatischen Start.  
- Konfiguriert `kubectl` f√ºr den aktuellen Benutzer.  

#### **Manuelle Installation (f√ºr mehr Kontrolle)**  
1. **K3s herunterladen:**  
   ```bash
   wget https://github.com/k3s-io/k3s/releases/download/v1.27.4%2Bk3s1/k3s
   chmod +x k3s
   sudo mv k3s /usr/local/bin/
   ```  
2. **Service einrichten:**  
   ```bash
   sudo k3s server &
   ```  

### **macOS (mit Multipass oder Docker)**  
#### **Option 1: Multipass (Ubuntu-VM)**  
```bash
multipass launch --name k3s-node --mem 2G --disk 10G
multipass exec k3s-node -- bash -c "curl -sfL https://get.k3s.io | sh -"
```  

#### **Option 2: Docker (k3d)**  
```bash
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
k3d cluster create mycluster --image rancher/k3s:v1.27.4-k3s1
```  

### **Windows (WSL2)**  
1. **WSL2 aktivieren:**  
   ```powershell
   wsl --install -d Ubuntu
   ```  
2. **K3s in WSL installieren:**  
   ```bash
   curl -sfL https://get.k3s.io | sh -
   ```  

---

## **2.3 K3s mit einem Befehl installieren**  

### **Single-Node-Cluster (All-in-One)**  
```bash
curl -sfL https://get.k3s.io | sh -
```  
- **Funktionsweise:**  
  - L√§dt das K3s-Installationsskript herunter.  
  - Installiert K3s als **Server mit eingebettetem SQLite**.  
  - Erstellt eine `kubeconfig`-Datei unter `/etc/rancher/k3s/k3s.yaml`.  

### **Multi-Node-Cluster (Agent-Modus)**  
1. **Master-Node einrichten:**  
   ```bash
   curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
   ```  
2. **Worker-Node hinzuf√ºgen:**  
   ```bash
   curl -sfL https://get.k3s.io | K3S_URL=https://<MASTER_IP>:6443 K3S_TOKEN=<NODE_TOKEN> sh -
   ```  
   - **`<MASTER_IP>`:** IP des Master-Knotens.  
   - **`<NODE_TOKEN>`:** Token aus `/var/lib/rancher/k3s/server/node-token` auf dem Master.  

---

## **2.4 K3s ohne Root-Rechte (MicroK8s-Vergleich)**  
K3s l√§uft standardm√§√üig als **root**. F√ºr eingeschr√§nkte Benutzer:  

### **Installation als Nicht-Root-Benutzer**  
```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_BIN_DIR=~/.local/bin sh -
export PATH=$PATH:~/.local/bin
```  

### **Vergleich mit MicroK8s**  
| Feature          | K3s                     | MicroK8s                |  
|------------------|-------------------------|-------------------------|  
| **Installation** | Einzeiler (`curl`)      | Snap-Paket (`snap install microk8s`) |  
| **Ressourcen**   | ~100 MB RAM             | ~500 MB RAM             |  
| **Root n√∂tig?**  | Optional (mit `INSTALL_K3S_BIN_DIR`) | Nein (Snap) |  

---

## **2.5 K3s mit Docker oder containerd**  
K3s unterst√ºtzt mehrere **Container Runtimes**:  

### **Standard: containerd**  
- **Vorteile:**  
  - Leichtgewichtiger als Docker.  
  - Direkte Integration mit Kubernetes.  

### **Docker als Runtime nutzen**  
1. **Docker installieren:**  
   ```bash
   sudo apt install docker.io
   ```  
2. **K3s mit Docker starten:**  
   ```bash
   curl -sfL https://get.k3s.io | sh -s - --docker
   ```  

### **Wechsel der Runtime nachtr√§glich**  
- **Zu containerd:**  
  ```bash
  sudo systemctl stop k3s
  sudo k3s server --container-runtime-endpoint containerd &
  ```  

---

### **Zusammenfassung**  
- K3s kann auf **Linux, macOS und Windows (WSL2)** installiert werden.  
- **Einzeilige Installation** f√ºr schnelle Einrichtung.  
- **Multi-Node-Cluster** m√∂glich mit Master/Worker-Architektur.  
- **Docker oder containerd** als Container-Runtime w√§hlbar.  

Im n√§chsten Abschnitt folgen die **grundlegenden K3s-Befehle und Konzepte**.



# **3. Grundlegende K3s-Konzepte und -Befehle**

## **3.1 Kubectl-Grundlagen**
### **kubectl-Konfiguration**
Die Hauptkonfigurationsdatei f√ºr kubectl befindet sich unter:
```
~/.kube/config
```
Bei K3s wird diese automatisch erstellt unter:
```
/etc/rancher/k3s/k3s.yaml
```

**Wichtige Konfigurationsbefehle:**
```bash
# Aktuellen Kontext anzeigen
kubectl config current-context

# Verf√ºgbare Kontexte auflisten
kubectl config get-contexts

# Kontext wechseln
kubectl config use-context <context-name>
```

### **Essentielle kubectl-Befehle**
| Befehl | Beschreibung | Beispiel |
|--------|-------------|----------|
| `get` | Ressourcen anzeigen | `kubectl get pods -A` |
| `describe` | Detaillierte Ressourceninfo | `kubectl describe pod nginx` |
| `logs` | Container-Logs anzeigen | `kubectl logs -f pod/nginx` |
| `exec` | Befehl in Container ausf√ºhren | `kubectl exec -it nginx -- bash` |
| `apply` | Manifest anwenden | `kubectl apply -f deploy.yaml` |
| `delete` | Ressource l√∂schen | `kubectl delete pod nginx` |

## **3.2 K3s-spezifische Komponenten**
### **Eingebettete Datenbank (SQLite)**
K3s verwendet standardm√§√üig SQLite statt etcd:
- Vorteile: Einfacher Setup, geringerer Ressourcenverbrauch
- Nachteile: Kein HA ohne externe DB
- Konfiguration:
  ```bash
  # Mit externer MySQL-DB starten
  k3s server --datastore-endpoint="mysql://username:password@tcp(hostname:3306)/database-name"
  ```

### **K3s Server und Agent**
- **Server-Komponente**:
  ```bash
  # Als Server starten
  k3s server

  # Mit bestimmten Optionen
  k3s server --disable traefik --cluster-init
  ```

- **Agent-Komponente**:
  ```bash
  # Als Agent einem Cluster beitreten
  k3s agent --server https://<master-ip>:6443 --token <node-token>
  ```

### **Integrierter Ingress-Controller (Traefik)**
Standardm√§√üig aktiviert:
```bash
# Status pr√ºfen
kubectl -n kube-system get pods -l app.kubernetes.io/name=traefik

# Deaktivieren bei Installation
curl -sfL https://get.k3s.io | sh -s - --disable traefik
```

## **3.3 Deployment von Pods und Services**
### **Beispiel-YAML-Manifest (nginx)**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

**Anwendung des Manifests:**
```bash
# Deployment erstellen
kubectl apply -f nginx-deployment.yaml

# Status pr√ºfen
kubectl get deployments
kubectl get pods
```

### **Service erstellen**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```

## **3.4 Helm unter K3s nutzen**
### **Helm Installation**
```bash
# Helm installieren
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Helm-Repositories hinzuf√ºgen
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
```

### **Beispiel: WordPress installieren**
```bash
# WordPress installieren
helm install my-wordpress bitnami/wordpress

# Installierte Releases anzeigen
helm list
```

### **K3s-spezifische Helm-Aspekte**
- Keine besonderen Anforderungen f√ºr K3s
- Tillerless Mode (Helm 3) funktioniert standardm√§√üig
- Persistent Volumes werden automatisch √ºber local-path-provisioner bereitgestellt

**Tipp:** F√ºr Produktionsumgebungen immer `--atomic` Flag verwenden:
```bash
helm upgrade --install --atomic my-app ./my-chart
```





# **4. Netzwerk und Storage in K3s**

## **4.1 Netzwerkkonfiguration**

### **Netzwerkmodell in K3s**
K3s verwendet standardm√§√üig **Flannel** als CNI (Container Network Interface) mit VXLAN-Backend:
- Automatische IP-Zuweisung f√ºr Pods (10.42.0.0/16)
- Node-to-Node Kommunikation √ºber Port 8472/UDP (VXLAN)
- Service-Netzwerk: 10.43.0.0/16

**Konfigurationsoptionen:**
```bash
# Mit alternativem CNI starten
k3s server --flannel-backend=host-gw  # F√ºr bessere Performance in lokalen Netzen
k3s server --disable-network-policy   # Calico Network Policies deaktivieren
```

### **Service-Typen im √úberblick**
| Typ | Beschreibung | K3s-Besonderheit |
|------|-------------|------------------|
| ClusterIP | Interne IP (Standard) | - |
| NodePort | Port auf allen Nodes | Bereich: 30000-32767 |
| LoadBalancer | Externer Zugriff | klipper-lb als Lightweight-LB |
| ExternalName | DNS CNAME | - |

### **Integrierter Ingress-Controller (Traefik)**
Standardkonfiguration:
- Automatische Let's Encrypt-Zertifikate (ACME)
- Ingress-Routen unter Port 80/443
- Dashboard unter `<node-ip>:9000`

**Beispiel-Ingress-Ressource:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
spec:
  tls:
  - hosts:
    - example.com
    secretName: example-tls
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

## **4.2 Persistent Storage**

### **Local Path Provisioner**
Standard-Storage-Provider in K3s:
- Erstellt PVs automatisch unter `/var/lib/rancher/k3s/storage`
- Keine zus√§tzliche Konfiguration n√∂tig

**Beispiel-PVC:**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
```

### **Externe Storage-L√∂sungen**
**NFS einbinden:**
1. NFS-Server vorbereiten
2. StorageClass erstellen:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-sc
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
```

**Longhorn installieren (HA Storage):**
```bash
helm repo add longhorn https://charts.longhorn.io
helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace
```

## **4.3 DNS und Service Discovery**

### **CoreDNS in K3s**
- Vorinstalliert als Cluster-DNS
- Konfiguration unter `/var/lib/rancher/k3s/server/manifests/coredns.yaml`
- Aufl√∂sung von Services als `<service>.<namespace>.svc.cluster.local`

**DNS-Konfiguration anpassen:**
```bash
# Custom DNS-Server hinzuf√ºgen
k3s server --cluster-dns 8.8.8.8 --cluster-domain mycluster.local
```

### **Service Discovery Patterns**
1. **Umgebungsvariablen**: Automatisch in Pods injiziert
   ```bash
   # Beispielvariablen:
   NGINX_SERVICE_HOST=10.43.100.10
   NGINX_SERVICE_PORT=80
   ```
2. **DNS-Lookup**: Empfohlene Methode
   ```bash
   nslookup nginx-service.default.svc.cluster.local
   ```

## **4.4 Erweiterte Netzwerkfeatures**

### **Network Policies**
Aktivierung in K3s:
```bash
k3s server --enable-network-policy
```

**Beispiel-Policy:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-access
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: api
    ports:
    - protocol: TCP
      port: 5432
```

### **Metallb Integration (Alternative zu klipper-lb)**
Installation:
```bash
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml
```

Konfiguration:
```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.1.100-192.168.1.200
```

## **4.5 Troubleshooting Netzwerkprobleme**

**H√§ufige Probleme und L√∂sungen:**
1. **Pods k√∂nnen sich nicht verbinden**
   ```bash
   kubectl run -it --rm --image=alpine network-test -- sh
   ping <target-ip>
   ```
2. **DNS-Aufl√∂sung fehlgeschlagen**
   ```bash
   kubectl get pods -n kube-system -l k8s-app=kube-dns
   kubectl logs -n kube-system <coredns-pod>
   ```
3. **Port-Konflikte**
   ```bash
   netstat -tulnp | grep <port>
   ```

**Diagnosetools:**
- `kubectl get endpoints` - √úberpr√ºfung der Service-Endpunkte
- `kubectl describe svc <service>` - Detaillierte Service-Informationen
- `tcpdump -i any -n port 8472` - Flannel Netzwerkverkehr analysieren




# **5. Sicherheit in K3s**

## **5.1 Authentifizierung und Autorisierung**

### **RBAC (Role-Based Access Control) Grundlagen**
K3s implementiert standardm√§√üig RBAC mit folgenden Komponenten:
- **ServiceAccounts**: Identit√§ten f√ºr Pods (`default` ServiceAccount automatisch erstellt)
- **Roles**: Namespace-beschr√§nkte Berechtigungen
- **ClusterRoles**: Clusterweite Berechtigungen
- **RoleBindings/ClusterRoleBindings**: Zuweisung von Rollen

**Beispiel-Rolle erstellen:**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

**ServiceAccount mit Rolle verbinden:**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: default
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

## **5.2 TLS und Zertifikatsverwaltung**

### **K3s Zertifikatsarchitektur**
- Automatisch generierte Zertifikate unter `/var/lib/rancher/k3s/server/tls`
- Hauptkomponenten:
  - `server-ca.crt`: CA f√ºr Server-Zertifikate
  - `client-ca.crt`: CA f√ºr Client-Zertifikate
  - `serving-kube-apiserver.crt`: API-Server-Zertifikat

**Zertifikatsrotation:**
```bash
# Manuelle Rotation
k3s certificate rotate

# Automatische Rotation (ab K3s v1.21+)
k3s server --rotate-server-certificates
```

### **Let's Encrypt Integration**
Mit Traefik Ingress:
```yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: secure-app
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`example.com`)
    kind: Rule
    services:
    - name: app-service
      port: 80
  tls:
    certResolver: letsencrypt
```

## **5.3 Pod-Sicherheitsrichtlinien**

### **Pod Security Standards**
K3s unterst√ºtzt drei vordefinierte Niveaus:
1. **Privileged** (uneingeschr√§nkt)
2. **Baseline** (minimale Einschr√§nkungen)
3. **Restricted** (strikteste Richtlinien)

**Beispiel-Implementierung:**
```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
```

## **5.4 Netzwerksicherheit**

### **Network Policies**
Aktivierung in K3s:
```bash
k3s server --enable-network-policy
```

**Beispiel-Policy f√ºr mehrschichtige App:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24
    ports:
    - protocol: TCP
      port: 80
```

## **5.5 Secrets Management**

### **Native Kubernetes Secrets**
```bash
# Secret erstellen
kubectl create secret generic db-creds \
  --from-literal=username=admin \
  --from-literal=password=S3cr3t!

# In Pod einbinden
env:
- name: DB_USER
  valueFrom:
    secretKeyRef:
      name: db-creds
      key: username
```

### **Externe Secrets-L√∂sungen**
**Vault Integration:**
1. Vault installieren
2. Secrets Operator deployen:
```bash
helm install secrets-external-secrets external-secrets/external-secrets
```

## **5.6 Compliance und Auditing**

### **K3s Audit-Logging**
Konfiguration unter `/etc/rancher/k3s/audit-policy.yaml`:
```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
```

**Aktivierung:**
```bash
k3s server --kube-apiserver-arg=audit-policy-file=/etc/rancher/k3s/audit-policy.yaml \
           --kube-apiserver-arg=audit-log-path=/var/log/k3s-audit.log
```

## **5.7 H√§rtung von K3s**

### **Best Practices f√ºr Produktionscluster**
1. **API-Server absichern:**
   ```bash
   k3s server --kube-apiserver-arg=anonymous-auth=false \
              --kube-apiserver-arg=enable-admission-plugins=NodeRestriction,PodSecurityPolicy
   ```
2. **etcd Verschl√ºsselung (bei externer DB):**
   ```yaml
   apiVersion: apiserver.config.k8s.io/v1
   kind: EncryptionConfiguration
   resources:
     - resources:
         - secrets
       providers:
         - aescbc:
             keys:
               - name: key1
                 secret: <base64-encoded-32-byte-key>
   ```
3. **Regelm√§√üige Updates:**
   ```bash
   k3s-upgrade.sh --channel stable
   ```

### **Sicherheits√ºberpr√ºfungen**
```bash
# Mit kube-bench testen
docker run --rm --pid=host -v /etc:/etc:ro -v /var:/var:ro aquasec/kube-bench:latest \
  --benchmark cis-1.6
```

**Wichtigste Sicherheits-Checks:**
1. API-Server Authentifizierung
2. etcd Verschl√ºsselung
3. Container Runtime Security
4. Netzwerkisolation
5. Pod Security Policies

Diese umfassenden Sicherheitsma√ünahmen helfen, K3s-Cluster gegen Angriffe zu h√§rten und Compliance-Anforderungen zu erf√ºllen.



# **6. Skalierung und Hochverf√ºgbarkeit (HA) mit K3s**

## **6.1 Multi-Server-Cluster Architektur**

### **HA-Komponenten in K3s**
K3s unterst√ºtzt Hochverf√ºgbarkeit durch:
- **Mehrere Server Nodes** (Master)
- **Externe Datenbank** (MySQL, PostgreSQL, etcd)
- **Embedded etcd** (ab K3s v1.19)

**Architekturvarianten:**
1. **Single-Server mit SQLite** (Nicht HA)
2. **HA mit externer DB** (Empfohlen)
3. **HA mit embedded etcd** (Experimentell)

### **Datenbankoptionen im Vergleich**
| Datenbank | Setup-Komplexit√§t | Performance | Empfohlen f√ºr |
|-----------|------------------|------------|--------------|
| SQLite | Einfach | Niedrig | Entwicklung |
| MySQL | Mittel | Hoch | Produktion |
| PostgreSQL | Mittel | Hoch | Produktion |
| etcd | Komplex | Sehr hoch | Gro√üe Cluster |

## **6.2 Praktische HA-Implementierung**

### **Schritt-f√ºr-Schritt HA-Setup mit MySQL**
1. **MySQL-Datenbank vorbereiten**:
   ```sql
   CREATE DATABASE k3s_cluster;
   CREATE USER 'k3s_user'@'%' IDENTIFIED BY 'securepassword';
   GRANT ALL PRIVILEGES ON k3s_cluster.* TO 'k3s_user'@'%';
   ```

2. **Ersten Server starten**:
   ```bash
   k3s server \
     --datastore-endpoint="mysql://k3s_user:securepassword@tcp(mysql-server:3306)/k3s_cluster" \
     --cluster-init
   ```

3. **Weitere Server hinzuf√ºgen**:
   ```bash
   k3s server \
     --datastore-endpoint="mysql://k3s_user:securepassword@tcp(mysql-server:3306)/k3s_cluster" \
     --server https://<first-server-ip>:6443
   ```

4. **Worker Nodes verbinden** (wie bei Einzelserver):
   ```bash
   K3S_TOKEN=<token> k3s agent --server https://<any-server-ip>:6443
   ```

### **Load Balancing f√ºr die API-Server**
**Empfohlene L√∂sungen:**
1. **Round-Robin DNS**
2. **HAProxy-Konfiguration**:
   ```conf
   frontend k3s-api
     bind *:6443
     mode tcp
     default_backend k3s-servers

   backend k3s-servers
     mode tcp
     balance roundrobin
     server server1 192.168.1.101:6443 check
     server server2 192.168.1.102:6443 check
     server server3 192.168.1.103:6443 check
   ```

## **6.3 Automatische Skalierung**

### **Horizontal Pod Autoscaler (HPA)**
**Beispielimplementierung:**
1. **Metriken-Server installieren**:
   ```bash
   kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
   ```

2. **HPA erstellen**:
   ```yaml
   apiVersion: autoscaling/v2
   kind: HorizontalPodAutoscaler
   metadata:
     name: web-app-hpa
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: Deployment
       name: web-app
     minReplicas: 2
     maxReplicas: 10
     metrics:
     - type: Resource
       resource:
         name: cpu
         target:
           type: Utilization
           averageUtilization: 50
   ```

### **Cluster Autoscaler mit K3s**
**F√ºr Cloud-Umgebungen**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.22.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --nodes=1:10:k3s-worker-group
```

## **6.4 Backup und Disaster Recovery**

### **K3s Backup-Strategien**
1. **Datenbank-Backups**:
   ```bash
   # MySQL Dump
   mysqldump -u k3s_user -p k3s_cluster > k3s_backup_$(date +%F).sql
   ```

2. **ETCD Snapshots** (bei embedded etcd):
   ```bash
   k3s etcd-snapshot save --name pre-upgrade-snapshot
   ```

3. **Manifest-Backup**:
   ```bash
   kubectl get all --all-namespaces -o yaml > cluster-state_$(date +%F).yaml
   ```

### **Wiederherstellungsprozedur**
**F√ºr MySQL-Backend**:
1. Datenbank wiederherstellen
2. Server mit gleichem Endpoint starten:
   ```bash
   k3s server \
     --datastore-endpoint="mysql://k3s_user:securepassword@tcp(mysql-server:3306)/k3s_cluster"
   ```

**F√ºr etcd-Snapshots**:
```bash
k3s server \
  --cluster-init \
  --cluster-reset \
  --etcd-snapshot-restore=pre-upgrade-snapshot
```

## **6.5 Leistungsoptimierung**

### **K3s Performance Tuning**
1. **Server-Parameter**:
   ```bash
   k3s server \
     --kube-apiserver-arg="http2-max-streams-per-connection=1000" \
     --kubelet-arg="max-pods=250"
   ```

2. **Ressourcenlimits**:
   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: optimized-app
   spec:
     containers:
     - name: app
       image: nginx
       resources:
         limits:
           cpu: "1"
           memory: "512Mi"
         requests:
           cpu: "0.5"
           memory: "256Mi"
   ```

### **Monitoring der Cluster-Kapazit√§t**
**Wichtige Metriken:**
```bash
watch -n 5 'kubectl top nodes && kubectl get pods -A -o wide'
```

**Kapazit√§tsplanung:**
```bash
kubectl describe nodes | grep -A 5 "Allocatable"
```

## **6.6 Praxisbeispiel: Produktionscluster**

### **Empfohlene HA-Architektur f√ºr Produktion**
```
                          +-----------------+
                          |   Load Balancer |
                          |   (HAProxy/Nginx) |
                          +--------+--------+
                                   |
         +-------------------------+-------------------------+
         |                         |                         |
+--------+--------+       +--------+--------+       +--------+--------+
|   K3s Server 1  |       |   K3s Server 2  |       |   K3s Server 3  |
| (Master/Worker) |       | (Master/Worker) |       | (Master/Worker) |
+--------+--------+       +--------+--------+       +--------+--------+
         |                         |                         |
         +-------------------------+-------------------------+
                                   |
                          +--------+--------+
                          |  Externe DB     |
                          |  (MySQL Cluster) |
                          +-----------------+
```

### **Bereitstellungsskript f√ºr HA-Cluster**
```bash
#!/bin/bash
# HA-Cluster mit 3 Servern und externer DB

DB_ENDPOINT="mysql://k3s_user:password@tcp(db.example.com:3306)/k3s_cluster"

# Server 1 (Cluster Initiator)
ssh server1 "curl -sfL https://get.k3s.io | sh -s - server \
  --datastore-endpoint=\"${DB_ENDPOINT}\" \
  --cluster-init \
  --disable traefik \
  --tls-san cluster.example.com"

# Server 2+3 (Join Cluster)
for NODE in server2 server3; do
  ssh $NODE "curl -sfL https://get.k3s.io | sh -s - server \
    --datastore-endpoint=\"${DB_ENDPOINT}\" \
    --server https://server1:6443 \
    --disable traefik \
    --tls-san cluster.example.com"
done

# Worker Nodes
for NODE in worker{1..5}; do
  ssh $NODE "curl -sfL https://get.k3s.io | K3S_URL=https://server1:6443 K3S_TOKEN=<token> sh -"
done
```

Diese umfassende Anleitung erm√∂glicht den Aufbau eines hochverf√ºgbaren, skalierbaren K3s-Clusters f√ºr Produktionsumgebungen mit allen notwendigen Komponenten f√ºr Betrieb und Wartung.



# **7. Monitoring und Logging in K3s**

## **7.1 Metriken-√úberwachung mit Prometheus und Grafana**

### **Prometheus-Operator Installation**
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
```

**Wichtige Komponenten:**
- **Prometheus Server**: Sammelt Metriken (Port 9090)
- **Alertmanager**: Alarmierung (Port 9093)
- **Grafana**: Visualisierung (Port 3000)
- **Node Exporter**: Systemmetriken

### **K3s-spezifische Metriken**
1. **Embedded Metrics Server aktivieren**:
   ```bash
   k3s server --kubelet-arg="address=0.0.0.0" --kubelet-arg="read-only-port=10255"
   ```
2. **Custom Dashboards f√ºr K3s**:
   ```bash
   kubectl apply -f https://raw.githubusercontent.com/rancher/k3s/master/grafana-dashboard.yaml
   ```

## **7.2 Log-Management mit Loki und Fluentd**

### **Loki Stack installieren**
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack \
  --namespace logging \
  --create-namespace \
  --set fluent-bit.enabled=true \
  --set promtail.enabled=false
```

**Log Pipeline:**
```
FluentBit (DaemonSet) ‚Üí Loki (Log-Speicher) ‚Üí Grafana (Visualisierung)
```

### **K3s-Logkonfiguration**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [INPUT]
        Name tail
        Path /var/log/containers/*.log
        Parser docker
        Tag kube.*
        Mem_Buf_Limit 5MB
        Skip_Long_Lines On

    [OUTPUT]
        Name loki
        Match *
        Host loki.logging.svc
        Port 3100
        Labels {job="fluent-bit"}
```

## **7.3 K3s-Dashboard L√∂sungen**

### **Rancher UI Integration**
```bash
helm install rancher rancher-latest/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=rancher.example.com \
  --set bootstrapPassword=admin
```

### **Kubernetes Dashboard (Standard)**
```bash
GITHUB_URL=https://github.com/kubernetes/dashboard/releases
VERSION=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION}/aio/deploy/recommended.yaml
```

**Access Token erstellen:**
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

## **7.4 Erweiterte √úberwachungstechniken**

### **Distributed Tracing mit Jaeger**
```bash
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm install jaeger jaegertracing/jaeger \
  --namespace observability \
  --create-namespace
```

### **Custom Metrics f√ºr HPA**
1. **Prometheus Adapter installieren**:
   ```bash
   helm install prometheus-adapter prometheus-community/prometheus-adapter \
     --namespace monitoring \
     --set prometheus.url=http://prometheus-server.monitoring.svc
   ```
2. **HPA mit Custom Metriken konfigurieren**:
   ```yaml
   - type: Pods
     pods:
       metric:
         name: http_requests_per_second
       target:
         type: AverageValue
         averageValue: 100
   ```

## **7.5 Troubleshooting-Tools**

### **K9s Terminal UI**
```bash
curl -sS https://webinstall.dev/k9s | bash
```

**Tastaturk√ºrzel:**
- `:pod` - Pod-Ansicht
- `:svc` - Service-Ansicht
- `Ctrl-d` - Describe
- `Shift-f` - Port Forward

### **Network Debugging**
```bash
kubectl run -it --rm --image=nicolaka/netshoot debug-pod -- bash
# Im Container:
tcpdump -i any -n port 8472  # Flannel Traffic
dig kubernetes.default.svc.cluster.local  # DNS Test
```

## **7.6 Langzeitarchivierung von Logs**

### **ELK Stack Integration**
```bash
helm install elasticsearch elastic/elasticsearch \
  --namespace logging \
  --create-namespace \
  --set replicas=3

helm install kibana elastic/kibana \
  --namespace logging \
  --set service.type=NodePort
```

### **S3-Backup f√ºr Loki**
```yaml
loki:
  config:
    schema_config:
      configs:
      - from: 2020-10-24
        store: boltdb-shipper
        object_store: aws
        schema: v11
        index:
          prefix: index_
          period: 24h
    storage_config:
      aws:
        s3: s3://<ACCESS_KEY>:<SECRET_KEY>@<ENDPOINT>/<BUCKET>
        s3forcepathstyle: true
```

## **7.7 Praxisbeispiel: Full-Stack-Monitoring**

### **Komplettes √úberwachungsdashboard**
```bash
git clone https://github.com/grafana/helm-charts
helm install grafana grafana/grafana \
  --namespace monitoring \
  --set persistence.enabled=true \
  --set adminPassword=secret \
  --set datasources."datasources\.yaml".apiVersion=1 \
  --set datasources."datasources\.yaml".datasources[0].name=Prometheus \
  --set datasources."datasources\.yaml".datasources[0].type=prometheus \
  --set datasources."datasources\.yaml".datasources[0].url=http://prometheus-server.monitoring.svc \
  --set datasources."datasources\.yaml".datasources[0].access=proxy
```

**Dashboard-Importe:**
1. Kubernetes Cluster (ID: 315)
2. K3s Resource Usage (ID: 13770)
3. Loki Logs (ID: 13439)

Diese umfassende Monitoring-L√∂sung erm√∂glicht die vollst√§ndige Observability von K3s-Clustern in Produktionsumgebungen mit allen notwendigen Komponenten f√ºr Metriken, Logs und Traces.



# **8. CI/CD mit K3s**

## **8.1 GitOps-Strategien f√ºr K3s**

### **ArgoCD Installation und Konfiguration**
```bash
helm repo add argo https://argoproj.github.io/argo-helm
helm install argocd argo/argo-cd \
  --namespace argocd \
  --create-namespace \
  --set server.service.type=LoadBalancer \
  --set configs.params.server.insecure=true
```

**K3s-spezifische Anpassungen:**
```yaml
# argocd-cm.yaml
data:
  kustomize.buildOptions: "--load-restrictor LoadRestrictionsNone"
  applications.resourceTrackingMethod: annotation
```

### **FluxCD als Alternative**
```bash
flux install \
  --namespace=flux-system \
  --components-extra=image-reflector-controller,image-automation-controller
```

**K3s Integration:**
```bash
flux create source git k3s-apps \
  --url=https://github.com/your/repo \
  --branch=main \
  --interval=1m

flux create kustomization k3s-prod \
  --source=k3s-apps \
  --path="./clusters/prod" \
  --prune=true \
  --interval=5m
```

## **8.2 Pipeline-Integration**

### **GitHub Actions f√ºr K3s**
```yaml
name: Deploy to K3s

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v1
      
    - name: Configure K3s access
      run: |
        mkdir -p ~/.kube
        echo "${{ secrets.KUBE_CONFIG }}" > ~/.kube/config
        
    - name: Deploy application
      run: kubectl apply -f k8s/
```

### **GitLab CI Pipeline**
```yaml
stages:
  - deploy

deploy_to_k3s:
  stage: deploy
  image: alpine/k8s:1.18.2
  script:
    - echo "$KUBE_CONFIG" > kubeconfig
    - export KUBECONFIG=./kubeconfig
    - kubectl apply -f manifests/
  only:
    - main
```

## **8.3 Fortgeschrittene Deployment-Strategien**

### **Blue/Green Deployments**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  ports:
  - port: 80
  selector:
    app: myapp
    track: blue

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      track: blue
  template:
    metadata:
      labels:
        app: myapp
        track: blue
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
```

**Switch-Befehl:**
```bash
kubectl patch service myapp -p '{"spec":{"selector":{"track":"green"}}}'
```

### **Canary Releases mit Flagger**
```bash
helm install flagger flagger/flagger \
  --namespace istio-system \
  --set prometheus.install=true \
  --set meshProvider=istio
```

**Canary-Analyse:**
```yaml
analysis:
  interval: 1m
  threshold: 5
  stepWeight: 10
  maxWeight: 50
  metrics:
  - name: request-success-rate
    thresholdRange:
      min: 99
    interval: 1m
  - name: request-duration
    thresholdRange:
      max: 500
    interval: 1m
```

## **8.4 Image Management**

### **Private Registry Integration**
```bash
k3s server \
  --private-registry "/etc/rancher/k3s/registries.yaml"
```

**registries.yaml Beispiel:**
```yaml
mirrors:
  docker.io:
    endpoint:
      - "https://mirror.example.com"
configs:
  "registry.example.com":
    auth:
      username: xxxxxx
      password: yyyyyy
```

### **Image Build mit BuildKit**
```dockerfile
# buildkitd.toml
[worker.containerd]
  namespace = "k8s.io"
```

**Build und Push:**
```bash
buildctl build \
  --frontend=dockerfile.v0 \
  --local context=. \
  --local dockerfile=. \
  --output type=image,name=registry.example.com/app:v1,push=true
```

## **8.5 Security Scanning**

### **Trivy in CI-Pipelines**
```yaml
- name: Scan image
  uses: aquasecurity/trivy-action@master
  with:
    image-ref: "registry.example.com/app:${{ github.sha }}"
    format: "table"
    exit-code: "1"
    severity: "CRITICAL,HIGH"
```

### **Gatekeeper Policy Controller**
```bash
helm install gatekeeper gatekeeper/gatekeeper \
  --namespace gatekeeper-system \
  --create-namespace
```

**Beispiel-Constraint:**
```yaml
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: require-team-label
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
  parameters:
    labels: ["team"]
```

## **8.6 Multi-Cluster Management**

### **Fleet f√ºr K3s-Cluster**
```bash
helm install fleet rancher-latest/fleet \
  --namespace fleet-system \
  --create-namespace \
  --set apiServerURL=https://k3s.example.com
```

**GitRepo-Ressource:**
```yaml
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: apps
  namespace: fleet-default
spec:
  repo: https://github.com/your/repo
  branch: main
  paths:
  - "deployments/*"
```

### **K3s Cluster API**
```bash
clusterctl init \
  --infrastructure docker \
  --control-plane k3s \
  --bootstrap k3s
```

## **8.7 Performance-Optimierung f√ºr CI/CD**

### **Kaniko Caching**
```yaml
- name: Build with Kaniko
  uses: google/kaniko@v1
  with:
    dockerfile: Dockerfile
    context: .
    cache: true
    cache-repo: registry.example.com/cache
```

### **K3s spezifische Tuning-Parameter**
```bash
k3s server \
  --kube-apiserver-arg="max-requests-inflight=1500" \
  --kube-controller-manager-arg="kube-api-burst=300" \
  --kubelet-arg="max-pods=250"
```

Diese umfassende CI/CD-Integration erm√∂glicht automatisierte, sichere und hochverf√ºgbare Bereitstellungen auf K3s-Clustern mit modernen GitOps-Praktiken und fortgeschrittenen Deployment-Strategien.



# **9. Troubleshooting und Optimierung in K3s**

## **9.1 Diagnose-Toolkit f√ºr K3s**

### **K3s-spezifische Logs**
```bash
# Server-Logs anzeigen
journalctl -u k3s -f

# Agent-Logs √ºberpr√ºfen
tail -f /var/log/syslog | grep k3s-agent

# Debug-Modus aktivieren
k3s server --debug
```

### **Kubectl-Debug-Container**
```bash
kubectl debug node/<node-name> -it --image=nicolaka/netshoot
```

### **K3s Statuspr√ºfung**
```bash
k3s check-config  # Systemvoraussetzungen pr√ºfen
k3s etcd-snapshot status  # Backup-Status
k3s kubectl get events -A --sort-by='.lastTimestamp'  # Cluster-Events
```

## **9.2 H√§ufige Probleme und L√∂sungen**

### **Startprobleme des K3s-Servers**
| Symptom | L√∂sung |
|---------|--------|
| "Failed to connect to database" | MySQL/PostgreSQL Verbindung pr√ºfen |
| "Port 6443 already in use" | `netstat -tulnp \| grep 6443` ‚Üí Konflikt beheben |
| "cgroup mounts failed" | `cgroup_enable=cpuset cgroup_memory=1` zu Boot-Params hinzuf√ºgen |

### **Netzwerkprobleme**
```bash
# Flannel-Interface pr√ºfen
ip addr show flannel.1

# DNS-Aufl√∂sung testen
kubectl run -it --rm --image=alpine:latest debug -- nslookup kubernetes.default

# Port-Weiterleitung testen
kubectl port-forward service/my-service 8080:80
```

## **9.3 Leistungsanalyse und Optimierung**

### **Ressourcen-Engp√§sse identifizieren**
```bash
# Top-Ressourcennutzer anzeigen
kubectl top pods --sort-by=cpu -A
kubectl top nodes

# Detaillierte Metriken
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes | jq .
```

### **Kernel-Parameter f√ºr K3s optimieren**
```bash
# /etc/sysctl.d/k3s.conf
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-iptables=1
vm.overcommit_memory=1
fs.inotify.max_user_instances=8192
```

### **Container Runtime Tuning**
```bash
# containerd Konfiguration
cat /var/lib/rancher/k3s/agent/etc/containerd/config.toml

# Docker Daemon optimieren
{
  "default-ulimits": {
    "nofile": {
      "Name": "nofile",
      "Hard": 65536,
      "Soft": 65536
    }
  },
  "max-concurrent-downloads": 10
}
```

## **9.4 Cluster-Reparaturverfahren**

### **Kritische Komponenten neustarten**
```bash
# K3s-Server komplett neustarten
systemctl restart k3s

# Einzelne Komponenten
k3s kubectl delete pod -n kube-system -l app=traefik
```

### **Zertifikatsrotation erzwingen**
```bash
k3s certificate rotate --all
rm -rf /var/lib/rancher/k3s/server/tls/*
systemctl restart k3s
```

### **Datenbank-Wiederherstellung**
```sql
-- MySQL Repair
REPAIR TABLE k3s.cluster;
OPTIMIZE TABLE k3s.cluster;
```

## **9.5 Debugging-Workflows**

### **Pod-Diagnose-Checkliste**
1. **Pod-Status pr√ºfen**: `kubectl describe pod`
2. **Logs analysieren**: `kubectl logs --previous`
3. **Ressourcenlimits**: `kubectl get pod -o json | jq '.spec.containers[].resources'`
4. **Netzwerkverbindungen**: `kubectl exec -it pod -- nc -zv service 80`
5. **DNS-Konfiguration**: `cat /etc/resolv.conf`

### **Node-Problemdiagnose**
```bash
# Node-Kapazit√§t
kubectl describe node | grep -A 10 Allocatable

# Systemd-Dienste
systemctl list-units --type=service | grep k3s

# Festplattennutzung
df -h /var/lib/rancher/k3s
```

## **9.6 Langzeitstabilit√§ts-Monitoring**

### **Prometheus-Alerts f√ºr K3s**
```yaml
groups:
- name: k3s.rules
  rules:
  - alert: K3sNodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="false"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.node }} is not ready"
  
  - alert: K3sHighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
    for: 15m
    labels:
      severity: warning
```

### **Log-basierte Alarme**
```bash
# Fluentbit-Alarmkonfiguration
[INPUT]
    Name tail
    Path /var/log/containers/k3s-server*.log
    Tag k3s_server
    Parser docker

[OUTPUT]
    Name http
    Match k3s_server
    Host alertmanager.example.com
    Port 9093
    URI /api/v1/alerts
    Format json
```

## **9.7 Best Practices f√ºr Produktionscluster**

### **Wartungs-Checkliste**
1. **Regelm√§√üige Updates**: `k3s-upgrade.sh --channel stable`
2. **Backup-Routine**: Automatisierte etcd-Snapshots
3. **Sicherheitsaudits**: `kube-bench --benchmark cis-1.6`
4. **Ressourcenbereinigung**: `kubectl get pods --field-selector status.phase==Succeeded -o name | xargs kubectl delete`

### **Performance-Benchmarking**
```bash
# Cluster-Test mit kube-burner
docker run -v $(pwd):/config cloudnativek8s/kube-burner:latest init \
  --config=/config/config.yml \
  --uuid=$(uuidgen) \
  --prometheus-url=http://prometheus-server:9090
```

Diese umfassende Troubleshooting-Anleitung bietet ein systematisches Vorgehen zur Problemdiagnose und Leistungsoptimierung in K3s-Clustern, mit spezifischen L√∂sungen f√ºr die h√§ufigsten Betriebsprobleme und bew√§hrten Verfahren f√ºr die Produktions√ºberwachung.